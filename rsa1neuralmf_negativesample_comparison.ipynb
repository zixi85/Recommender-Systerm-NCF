{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "di74PRcz9nmA"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.multiprocessing as mp\n",
        "mp.set_start_method('spawn', force=True)\n",
        "# import data as data\n",
        "# import model_evaluation as evaluation\n",
        "import torch.optim as optim\n",
        "import torch._dynamo\n",
        "import numpy as np\n",
        "import heapq\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "\n",
        "random.seed(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (user_id: [(movie_id, label)])"
      ],
      "metadata": {
        "id": "tPNNv1OB61MV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "lAFO8s3BoUsS"
      },
      "outputs": [],
      "source": [
        "def load_data_rate(filename, threshold=3, train_ratio=0.7, test_ratio=0.15):\n",
        "    \"\"\"\n",
        "    Load dataset and split data on a per-user basis.\n",
        "\n",
        "    Args:\n",
        "        filename (str): Path to the ratings file.\n",
        "        train_ratio (float): Percentage of interactions used for training.\n",
        "        test_ratio (float): Percentage of interactions used for testing.\n",
        "\n",
        "    Returns:\n",
        "        train_dict, val_dict, test_dict, movie_num, user_num\n",
        "    \"\"\"\n",
        "    user_ratings = {}  # Store each user's interactions (user_id: [(movie_id, label), ...])\n",
        "    movie_num = -1\n",
        "    user_num = -1\n",
        "\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
        "        for line in file:\n",
        "            user_id, movie_id, rating, _ = map(int, line.strip().split(\"::\"))\n",
        "            label = 1 if rating >= threshold else 0\n",
        "\n",
        "            if user_id not in user_ratings:\n",
        "                user_ratings[user_id] = []\n",
        "            user_ratings[user_id].append((movie_id, label))\n",
        "\n",
        "            # movie and user number\n",
        "            movie_num = max(movie_num, movie_id)\n",
        "            user_num = max(user_num, user_id)\n",
        "\n",
        "    train_dict, val_dict, test_dict = {}, {}, {}\n",
        "\n",
        "    ######### divide by users? cold start #######\n",
        "    # Divide each user's movie interactions by proportion\n",
        "    for user_id, interactions in user_ratings.items():\n",
        "        random.shuffle(interactions)  # shuffle\n",
        "\n",
        "        total_interactions = len(interactions)\n",
        "        train_end = int(train_ratio * total_interactions)\n",
        "        val_end = int((train_ratio + test_ratio) * total_interactions)\n",
        "\n",
        "        train_dict[user_id] = interactions[:train_end]\n",
        "        val_dict[user_id] = interactions[train_end:val_end]\n",
        "        test_dict[user_id] = interactions[val_end:]\n",
        "\n",
        "    return train_dict, val_dict, test_dict, movie_num, user_num"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# noninteract movies (for validation and test set to rank)"
      ],
      "metadata": {
        "id": "z9BP_ywI6_Nm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "z9F4p4DroUsT"
      },
      "outputs": [],
      "source": [
        "def get_input_data_nointeract(train_dict, non_interacted_movies):\n",
        "    user_input, movie_input, labels = [], [], []\n",
        "\n",
        "    for u, rate_list in train_dict.items():\n",
        "        # positive samples in train set\n",
        "        for movie_id, label in rate_list:\n",
        "            user_input.append(u)\n",
        "            movie_input.append(movie_id)\n",
        "            labels.append(label)\n",
        "\n",
        "        # collect all movies not interacted with user\n",
        "        non_interacted_items = non_interacted_movies.get(u, [])\n",
        "\n",
        "        # Add all non-interacted movies as negative samples with label 0\n",
        "        for movie_id in non_interacted_items:\n",
        "            user_input.append(u)\n",
        "            movie_input.append(movie_id)\n",
        "            labels.append(0)\n",
        "\n",
        "    return user_input, movie_input, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# get input data for training set with negative sampling"
      ],
      "metadata": {
        "id": "8_HQM9aT7SNv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "LaTCk9RJoUsT"
      },
      "outputs": [],
      "source": [
        "######### rate>=4 negative sample? interaction #######\n",
        "def get_input_data(train_dict, non_interacted_movies, negative_num):\n",
        "    user_input, movie_input, labels = [], [], []\n",
        "\n",
        "    for u, rate_list in train_dict.items():\n",
        "        # positive samples in train set\n",
        "        for movie_id, label in rate_list:\n",
        "            user_input.append(u)\n",
        "            movie_input.append(movie_id)\n",
        "            labels.append(label)\n",
        "\n",
        "        # collect all movies not interacted with user\n",
        "        non_interacted_items = non_interacted_movies.get(u, [])\n",
        "        # negative samples\n",
        "        if len(non_interacted_items) >= negative_num:\n",
        "            negative_samples = random.sample(non_interacted_items, negative_num)\n",
        "        else:\n",
        "            negative_samples = list(non_interacted_items) + random.choices(list(non_interacted_items), k=negative_num - len(non_interacted_items))\n",
        "\n",
        "        for movie_id in negative_samples:\n",
        "            user_input.append(u)\n",
        "            movie_input.append(movie_id)\n",
        "            labels.append(0)\n",
        "\n",
        "    return user_input, movie_input, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "1kPXu4XroUsT"
      },
      "outputs": [],
      "source": [
        "def get_non_interacted_movies(train_dict, val_dict, test_dict, movie_num):\n",
        "    non_interacted_movies = {}\n",
        "\n",
        "    for u in train_dict:\n",
        "        # Get the movies that the user has interacted with (including train, val, test)\n",
        "        interacted_movies = set(movie_id for movie_id, _ in train_dict.get(u, []))\n",
        "        if u in val_dict:\n",
        "            interacted_movies.update(movie_id for movie_id, _ in val_dict.get(u, []))\n",
        "        if u in test_dict:\n",
        "            interacted_movies.update(movie_id for movie_id, _ in test_dict.get(u, []))\n",
        "\n",
        "        # Get the movies that the user has not interacted with\n",
        "        all_movies = set(range(1, movie_num + 1))\n",
        "        non_interacted_movies[u] = list(all_movies - interacted_movies)\n",
        "\n",
        "    return non_interacted_movies"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NeuMF"
      ],
      "metadata": {
        "id": "Nux1qUXH7ayr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "7scibCrO9nmB"
      },
      "outputs": [],
      "source": [
        "class NeuMF(nn.Module):\n",
        "    def __init__(self, num_users, num_items, mf_dim=10, layers=[10]):\n",
        "        super(NeuMF, self).__init__()\n",
        "\n",
        "        # GMF Embeddings\n",
        "        self.user_embedding_gmf = nn.Embedding(num_users, mf_dim)\n",
        "        self.item_embedding_gmf = nn.Embedding(num_items, mf_dim)\n",
        "\n",
        "        # MLP Embeddings\n",
        "        self.user_embedding_mlp = nn.Embedding(num_users, layers[0] // 2)\n",
        "        self.item_embedding_mlp = nn.Embedding(num_items, layers[0] // 2)\n",
        "\n",
        "        # Initialize embedding weights\n",
        "        nn.init.normal_(self.user_embedding_gmf.weight, std=0.01)\n",
        "        nn.init.normal_(self.item_embedding_gmf.weight, std=0.01)\n",
        "        nn.init.normal_(self.user_embedding_mlp.weight, std=0.01)\n",
        "        nn.init.normal_(self.item_embedding_mlp.weight, std=0.01)\n",
        "\n",
        "        # MLP Layers\n",
        "        self.mlp_layers = nn.Sequential()\n",
        "        input_dim = layers[0]  # Initial input size (concatenated user & item embeddings)\n",
        "        for i in range(1, len(layers)):\n",
        "            self.mlp_layers.add_module(f\"fc{i}\", nn.Linear(input_dim, layers[i]))\n",
        "            self.mlp_layers.add_module(f\"relu{i}\", nn.ReLU())\n",
        "            input_dim = layers[i]\n",
        "\n",
        "        # Output layer: combines GMF and MLP outputs\n",
        "        self.fc_output = nn.Linear(mf_dim + layers[-1], 1)  # GMF (mf_dim) + MLP (last layer size)\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        \"\"\" Forward pass for NeuMF model \"\"\"\n",
        "\n",
        "        # GMF Forward Pass: Element-wise multiplication\n",
        "        user_latent_gmf = self.user_embedding_gmf(user_indices)\n",
        "        item_latent_gmf = self.item_embedding_gmf(item_indices)\n",
        "        gmf_out = torch.mul(user_latent_gmf, item_latent_gmf)  # Element-wise multiplication\n",
        "\n",
        "        # MLP Forward Pass: Concatenate embeddings and pass through MLP layers\n",
        "        user_latent_mlp = self.user_embedding_mlp(user_indices)\n",
        "        item_latent_mlp = self.item_embedding_mlp(item_indices)\n",
        "        mlp_input = torch.cat((user_latent_mlp, item_latent_mlp), dim=-1)  # Concatenation\n",
        "        mlp_out = self.mlp_layers(mlp_input)\n",
        "\n",
        "        # Combine GMF and MLP outputs\n",
        "        combined = torch.cat((gmf_out, mlp_out), dim=-1)\n",
        "        prediction = torch.sigmoid(self.fc_output(combined))  # Final prediction\n",
        "\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "PM5gklTGoUsU"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "c83zIoS6oUsU"
      },
      "outputs": [],
      "source": [
        "# def model_evaluation(model, val_dict, device, K=10):\n",
        "#     model.to(device)\n",
        "#     model.eval()\n",
        "#     user_input = []\n",
        "#     movie_input = []\n",
        "#     labels = []\n",
        "\n",
        "#     for u, interactions in val_dict.items():\n",
        "#         for movie_id, label in interactions:\n",
        "#             user_input.append(u)\n",
        "#             movie_input.append(movie_id)\n",
        "#             labels.append(label)\n",
        "\n",
        "#     user_input = torch.tensor(user_input, dtype=torch.long, device=device)\n",
        "#     movie_input = torch.tensor(movie_input, dtype=torch.long, device=device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         predictions = model(user_input, movie_input).squeeze(-1).cpu().numpy()\n",
        "\n",
        "#     predictions_dict = {}\n",
        "#     for u, m, score in zip(user_input.cpu().tolist(), movie_input.cpu().tolist(), predictions):\n",
        "#         if u not in predictions_dict:\n",
        "#             predictions_dict[u] = {}\n",
        "#         predictions_dict[u][m] = score\n",
        "\n",
        "#     precision_list = []\n",
        "#     recall_list = []\n",
        "\n",
        "#     for u, interactions in val_dict.items():\n",
        "#         pos_movies = {m for m, label in interactions if label == 1}\n",
        "#         if not pos_movies:\n",
        "#             continue\n",
        "\n",
        "#         if u not in predictions_dict:\n",
        "#             continue\n",
        "#         pred_scores = predictions_dict[u]\n",
        "\n",
        "#         top_k_items = np.array(sorted(pred_scores.keys(), key=lambda x: pred_scores[x], reverse=True))[:K]\n",
        "\n",
        "#         # Calculate Precision@10\n",
        "#         relevant_in_top_k = sum(1 for movie_id in top_k_items if movie_id in pos_movies)\n",
        "#         precision_at_10 = relevant_in_top_k / K\n",
        "#         precision_list.append(precision_at_10)\n",
        "\n",
        "#         # Calculate Recall@10\n",
        "#         recall_at_10 = relevant_in_top_k / len(pos_movies)\n",
        "#         recall_list.append(recall_at_10)\n",
        "\n",
        "#     # Calculate average Precision@10 and Recall@10\n",
        "#     avg_precision_at_10 = np.mean(precision_list) if precision_list else 0\n",
        "#     avg_recall_at_10 = np.mean(recall_list) if recall_list else 0\n",
        "\n",
        "#     # Calculate F1@10\n",
        "#     if avg_precision_at_10 + avg_recall_at_10 > 0:\n",
        "#         f1_at_10 = 2 * (avg_precision_at_10 * avg_recall_at_10) / (avg_precision_at_10 + avg_recall_at_10)\n",
        "#     else:\n",
        "#         f1_at_10 = 0\n",
        "\n",
        "#     return avg_precision_at_10, avg_recall_at_10, f1_at_10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "SwLXLy0Y9nmB"
      },
      "outputs": [],
      "source": [
        "def model_evaluation(model, val_dict, device, K=10):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    user_input = []\n",
        "    movie_input = []\n",
        "    labels = []\n",
        "\n",
        "    for u, interactions in val_dict.items():\n",
        "        for movie_id, label in interactions:\n",
        "            user_input.append(u)\n",
        "            movie_input.append(movie_id)\n",
        "            labels.append(label)\n",
        "\n",
        "    user_input = torch.tensor(user_input, dtype=torch.long, device=device)\n",
        "    movie_input = torch.tensor(movie_input, dtype=torch.long, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(user_input, movie_input).squeeze(-1).cpu().numpy()\n",
        "\n",
        "    predictions_dict = {}\n",
        "    for u, m, score in zip(user_input.cpu().tolist(), movie_input.cpu().tolist(), predictions):\n",
        "        if u not in predictions_dict:\n",
        "            predictions_dict[u] = {}\n",
        "        predictions_dict[u][m] = score\n",
        "\n",
        "    recall_list = []\n",
        "    ndcg_list = []\n",
        "\n",
        "    for u, interactions in val_dict.items():\n",
        "        pos_movies = {m for m, label in interactions if label == 1}\n",
        "        if not pos_movies:\n",
        "            continue\n",
        "\n",
        "        if u not in predictions_dict:\n",
        "            continue\n",
        "        pred_scores = predictions_dict[u]\n",
        "\n",
        "        top_k_items = np.array(sorted(pred_scores.keys(), key=lambda x: pred_scores[x], reverse=True))[:K]\n",
        "\n",
        "        recall = len(pos_movies.intersection(top_k_items)) / len(pos_movies)\n",
        "        recall_list.append(recall)\n",
        "\n",
        "        ndcg = calculate_ndcg(pos_movies, top_k_items, K)\n",
        "        ndcg_list.append(ndcg)\n",
        "\n",
        "    avg_recall = np.mean(recall_list) if recall_list else 0\n",
        "    avg_ndcg = np.mean(ndcg_list) if ndcg_list else 0\n",
        "\n",
        "    return avg_recall, avg_ndcg\n",
        "\n",
        "\n",
        "def calculate_ndcg(pos_movies, top_k_items, K):\n",
        "    \"\"\"\n",
        "    Calculate NDCG for the top-K recommended items.\n",
        "\n",
        "    Args:\n",
        "    - pos_movies: A set of relevant (ground truth) items for the user.\n",
        "    - top_k_items: A list of the top-K recommended items.\n",
        "    - K: The number of top items considered for evaluation.\n",
        "\n",
        "    Returns:\n",
        "    - NDCG score.\n",
        "    \"\"\"\n",
        "    K = min(K, len(top_k_items))  # Adjust K to avoid overestimation\n",
        "\n",
        "    # Compute DCG\n",
        "    dcg = sum(1 / np.log2(i + 2) for i, item in enumerate(top_k_items[:K]) if item in pos_movies)\n",
        "\n",
        "    # Compute IDCG (Ideal DCG)\n",
        "    ideal_hits = min(K, len(pos_movies))  # Can't be more than positive items\n",
        "    idcg = sum(1 / np.log2(i + 2) for i in range(ideal_hits))\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "BahobKD0oUsV"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NeuMF"
      ],
      "metadata": {
        "id": "pZaJgBtp4EfW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlKTCLzXoUsW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d0659c2-29c8-4448-8b0b-51e374e60f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.5259811670086105\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_dict, val_dict, test_dict, movie_num, user_num = load_data_rate('/content/ratings.dat', threshold=3)\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "\n",
        "negative_nums = [5]  # Different values of negative samples to test\n",
        "results = {}  # Dictionary to store results for each negative_num\n",
        "\n",
        "# Loop through each negative_num\n",
        "for negative_num in negative_nums:\n",
        "    # Update non-interacted movies list for the current negative_num\n",
        "    non_interacted_movies = get_non_interacted_movies(train_dict, val_dict, test_dict, movie_num)\n",
        "\n",
        "    # Get input data for training with the current negative_num\n",
        "    user_input, movie_input, labels = get_input_data(train_dict, non_interacted_movies, negative_num)\n",
        "\n",
        "    # Reinitialize the model, optimizer, and other components\n",
        "    model_ncf = NeuMF(user_num + 1, movie_num + 1, 10, [10, 16]).to(device)\n",
        "    model_ncf = torch.compile(model_ncf)\n",
        "    optimizer = optim.Adam(model_ncf.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=3, verbose=True)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()  # Loss function\n",
        "    scaler = torch.amp.GradScaler('cuda')  # Automatic mixed precision scaler\n",
        "\n",
        "    # Convert input data to tensors and load them to device\n",
        "    user_input = torch.tensor(user_input, dtype=torch.long).to(device)\n",
        "    movie_input = torch.tensor(movie_input, dtype=torch.long).to(device)\n",
        "    labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(user_input, movie_input, labels)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "    # Prepare validation data\n",
        "    val_user_input, val_movie_input, val_labels = get_input_data_nointeract(val_dict, non_interacted_movies)\n",
        "    val_dict = defaultdict(list)\n",
        "\n",
        "    # Organize validation data\n",
        "    for user_id, movie_id, label in zip(val_user_input, val_movie_input, val_labels):\n",
        "        val_dict[user_id].append((movie_id, label))\n",
        "\n",
        "    val_dict = dict(val_dict)\n",
        "\n",
        "    # Convert validation data to tensors\n",
        "    val_user_input = torch.tensor(val_user_input, dtype=torch.long).to(device)\n",
        "    val_movie_input = torch.tensor(val_movie_input, dtype=torch.long).to(device)\n",
        "    val_labels = torch.tensor(val_labels, dtype=torch.float32).to(device)\n",
        "\n",
        "    val_dataset = torch.utils.data.TensorDataset(val_user_input, val_movie_input, val_labels)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "    # Start training for the current negative_num\n",
        "    train_losses_ncf = []  # List to store training losses\n",
        "    val_losses_ncf = []  # List to store validation losses\n",
        "    recalls_ncf = []  # List to store recall scores\n",
        "    ndcgs_ncf = []  # List to store NDCG scores\n",
        "    patience = 10  # Patience for early stopping\n",
        "    counter = 0  # Counter for early stopping\n",
        "    best_val_loss = float('inf')  # Best validation loss\n",
        "\n",
        "    # Loop through epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        model_ncf.train()  # Set model to training mode\n",
        "        total_loss = 0\n",
        "\n",
        "        # Train the model with the training data\n",
        "        for batch_users, batch_items, batch_labels in dataloader:\n",
        "            batch_users = batch_users.to(device)\n",
        "            batch_items = batch_items.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                predictions = model_ncf(batch_users, batch_items)\n",
        "                loss = criterion(predictions, batch_labels.view(-1, 1))\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Training Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "        # Validate the model\n",
        "        model_ncf.eval()  # Set model to evaluation mode\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_users, batch_items, batch_labels in val_dataloader:\n",
        "                batch_users = batch_users.to(device)\n",
        "                batch_items = batch_items.to(device)\n",
        "                batch_labels = batch_labels.to(device)\n",
        "                predictions = model_ncf(batch_users, batch_items)\n",
        "                loss = criterion(predictions, batch_labels.view(-1, 1))\n",
        "                val_loss += loss.item()\n",
        "            val_loss_avg = val_loss / len(val_dataloader)\n",
        "            scheduler.step(val_loss_avg)  # Adjust learning rate based on validation loss\n",
        "            print(f\"Epoch {epoch + 1}, Validation Loss: {val_loss_avg}\")\n",
        "\n",
        "        # Save losses for later analysis\n",
        "        train_losses_ncf.append(total_loss / len(dataloader))\n",
        "        val_losses_ncf.append(val_loss_avg)\n",
        "\n",
        "        # Evaluate model's performance\n",
        "        recall, ndcg = model_evaluation(model_ncf, val_dict, device, K=10)\n",
        "        recalls_ncf.append(recall)\n",
        "        ndcgs_ncf.append(ndcg)\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss_avg < best_val_loss:\n",
        "            best_val_loss = val_loss_avg\n",
        "            counter = 0\n",
        "            # Save the best model\n",
        "            torch.save(model_ncf.state_dict(), f\"./best_model_ncf_{negative_num}.pth\")\n",
        "        else:\n",
        "            counter += 1\n",
        "            print(f\"Early Stopping Counter: {counter}/{patience}\")\n",
        "            if counter >= patience:\n",
        "                print(\"Early stopping triggered! Stopping training.\")\n",
        "                break\n",
        "\n",
        "    # Save the results for the current negative_num\n",
        "    results[negative_num] = {\n",
        "        \"train_losses\": train_losses_ncf,\n",
        "        \"val_losses\": val_losses_ncf,\n",
        "        \"recalls\": recalls_ncf,\n",
        "        \"ndcgs\": ndcgs_ncf\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1-R9Cr-nh0g"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_dict, val_dict, test_dict, movie_num, user_num = load_data_rate('/content/ratings.dat', threshold=3)\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "\n",
        "negative_nums = [10]  # Different values of negative samples to test\n",
        "\n",
        "# Loop through each negative_num\n",
        "for negative_num in negative_nums:\n",
        "    # Update non-interacted movies list for the current negative_num\n",
        "    non_interacted_movies = get_non_interacted_movies(train_dict, val_dict, test_dict, movie_num)\n",
        "\n",
        "    # Get input data for training with the current negative_num\n",
        "    user_input, movie_input, labels = get_input_data(train_dict, non_interacted_movies, negative_num)\n",
        "\n",
        "    # Reinitialize the model, optimizer, and other components\n",
        "    model_ncf = NeuMF(user_num + 1, movie_num + 1, 10, [10, 16]).to(device)\n",
        "    model_ncf = torch.compile(model_ncf)\n",
        "    optimizer = optim.Adam(model_ncf.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=3, verbose=True)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()  # Loss function\n",
        "    scaler = torch.amp.GradScaler('cuda')  # Automatic mixed precision scaler\n",
        "\n",
        "    # Convert input data to tensors and load them to device\n",
        "    user_input = torch.tensor(user_input, dtype=torch.long).to(device)\n",
        "    movie_input = torch.tensor(movie_input, dtype=torch.long).to(device)\n",
        "    labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(user_input, movie_input, labels)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "    # Prepare validation data\n",
        "    val_user_input, val_movie_input, val_labels = get_input_data_nointeract(val_dict, non_interacted_movies)\n",
        "    val_dict = defaultdict(list)\n",
        "\n",
        "    # Organize validation data\n",
        "    for user_id, movie_id, label in zip(val_user_input, val_movie_input, val_labels):\n",
        "        val_dict[user_id].append((movie_id, label))\n",
        "\n",
        "    val_dict = dict(val_dict)\n",
        "\n",
        "    # Convert validation data to tensors\n",
        "    val_user_input = torch.tensor(val_user_input, dtype=torch.long).to(device)\n",
        "    val_movie_input = torch.tensor(val_movie_input, dtype=torch.long).to(device)\n",
        "    val_labels = torch.tensor(val_labels, dtype=torch.float32).to(device)\n",
        "\n",
        "    val_dataset = torch.utils.data.TensorDataset(val_user_input, val_movie_input, val_labels)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "    # Start training for the current negative_num\n",
        "    train_losses_ncf = []  # List to store training losses\n",
        "    val_losses_ncf = []  # List to store validation losses\n",
        "    recalls_ncf = []  # List to store recall scores\n",
        "    ndcgs_ncf = []  # List to store NDCG scores\n",
        "    patience = 10  # Patience for early stopping\n",
        "    counter = 0  # Counter for early stopping\n",
        "    best_val_loss = float('inf')  # Best validation loss\n",
        "\n",
        "    # Loop through epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        model_ncf.train()  # Set model to training mode\n",
        "        total_loss = 0\n",
        "\n",
        "        # Train the model with the training data\n",
        "        for batch_users, batch_items, batch_labels in dataloader:\n",
        "            batch_users = batch_users.to(device)\n",
        "            batch_items = batch_items.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                predictions = model_ncf(batch_users, batch_items)\n",
        "                loss = criterion(predictions, batch_labels.view(-1, 1))\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Training Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "        # Validate the model\n",
        "        model_ncf.eval()  # Set model to evaluation mode\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_users, batch_items, batch_labels in val_dataloader:\n",
        "                batch_users = batch_users.to(device)\n",
        "                batch_items = batch_items.to(device)\n",
        "                batch_labels = batch_labels.to(device)\n",
        "                predictions = model_ncf(batch_users, batch_items)\n",
        "                loss = criterion(predictions, batch_labels.view(-1, 1))\n",
        "                val_loss += loss.item()\n",
        "            val_loss_avg = val_loss / len(val_dataloader)\n",
        "            scheduler.step(val_loss_avg)  # Adjust learning rate based on validation loss\n",
        "            print(f\"Epoch {epoch + 1}, Validation Loss: {val_loss_avg}\")\n",
        "\n",
        "        # Save losses for later analysis\n",
        "        train_losses_ncf.append(total_loss / len(dataloader))\n",
        "        val_losses_ncf.append(val_loss_avg)\n",
        "\n",
        "        # Evaluate model's performance\n",
        "        recall, ndcg = model_evaluation(model_ncf, val_dict, device, K=10)\n",
        "        recalls_ncf.append(recall)\n",
        "        ndcgs_ncf.append(ndcg)\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss_avg < best_val_loss:\n",
        "            best_val_loss = val_loss_avg\n",
        "            counter = 0\n",
        "            # Save the best model\n",
        "            torch.save(model_ncf.state_dict(), f\"./best_model_ncf_{negative_num}.pth\")\n",
        "        else:\n",
        "            counter += 1\n",
        "            print(f\"Early Stopping Counter: {counter}/{patience}\")\n",
        "            if counter >= patience:\n",
        "                print(\"Early stopping triggered! Stopping training.\")\n",
        "                break\n",
        "\n",
        "    # Save the results for the current negative_num\n",
        "    results[negative_num] = {\n",
        "        \"train_losses\": train_losses_ncf,\n",
        "        \"val_losses\": val_losses_ncf,\n",
        "        \"recalls\": recalls_ncf,\n",
        "        \"ndcgs\": ndcgs_ncf\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJR_xxeonslE"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_dict, val_dict, test_dict, movie_num, user_num = load_data_rate('/content/ratings.dat', threshold=3)\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "\n",
        "negative_nums = [15]  # Different values of negative samples to test\n",
        "\n",
        "# Loop through each negative_num\n",
        "for negative_num in negative_nums:\n",
        "    # Update non-interacted movies list for the current negative_num\n",
        "    non_interacted_movies = get_non_interacted_movies(train_dict, val_dict, test_dict, movie_num)\n",
        "\n",
        "    # Get input data for training with the current negative_num\n",
        "    user_input, movie_input, labels = get_input_data(train_dict, non_interacted_movies, negative_num)\n",
        "\n",
        "    # Reinitialize the model, optimizer, and other components\n",
        "    model_ncf = NeuMF(user_num + 1, movie_num + 1, 10, [10, 16]).to(device)\n",
        "    model_ncf = torch.compile(model_ncf)\n",
        "    optimizer = optim.Adam(model_ncf.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=3, verbose=True)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()  # Loss function\n",
        "    scaler = torch.amp.GradScaler('cuda')  # Automatic mixed precision scaler\n",
        "\n",
        "    # Convert input data to tensors and load them to device\n",
        "    user_input = torch.tensor(user_input, dtype=torch.long).to(device)\n",
        "    movie_input = torch.tensor(movie_input, dtype=torch.long).to(device)\n",
        "    labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(user_input, movie_input, labels)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "    # Prepare validation data\n",
        "    val_user_input, val_movie_input, val_labels = get_input_data_nointeract(val_dict, non_interacted_movies)\n",
        "    val_dict = defaultdict(list)\n",
        "\n",
        "    # Organize validation data\n",
        "    for user_id, movie_id, label in zip(val_user_input, val_movie_input, val_labels):\n",
        "        val_dict[user_id].append((movie_id, label))\n",
        "\n",
        "    val_dict = dict(val_dict)\n",
        "\n",
        "    # Convert validation data to tensors\n",
        "    val_user_input = torch.tensor(val_user_input, dtype=torch.long).to(device)\n",
        "    val_movie_input = torch.tensor(val_movie_input, dtype=torch.long).to(device)\n",
        "    val_labels = torch.tensor(val_labels, dtype=torch.float32).to(device)\n",
        "\n",
        "    val_dataset = torch.utils.data.TensorDataset(val_user_input, val_movie_input, val_labels)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "    # Start training for the current negative_num\n",
        "    train_losses_ncf = []  # List to store training losses\n",
        "    val_losses_ncf = []  # List to store validation losses\n",
        "    recalls_ncf = []  # List to store recall scores\n",
        "    ndcgs_ncf = []  # List to store NDCG scores\n",
        "    patience = 10  # Patience for early stopping\n",
        "    counter = 0  # Counter for early stopping\n",
        "    best_val_loss = float('inf')  # Best validation loss\n",
        "\n",
        "    # Loop through epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        model_ncf.train()  # Set model to training mode\n",
        "        total_loss = 0\n",
        "\n",
        "        # Train the model with the training data\n",
        "        for batch_users, batch_items, batch_labels in dataloader:\n",
        "            batch_users = batch_users.to(device)\n",
        "            batch_items = batch_items.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                predictions = model_ncf(batch_users, batch_items)\n",
        "                loss = criterion(predictions, batch_labels.view(-1, 1))\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Training Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "        # Validate the model\n",
        "        model_ncf.eval()  # Set model to evaluation mode\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_users, batch_items, batch_labels in val_dataloader:\n",
        "                batch_users = batch_users.to(device)\n",
        "                batch_items = batch_items.to(device)\n",
        "                batch_labels = batch_labels.to(device)\n",
        "                predictions = model_ncf(batch_users, batch_items)\n",
        "                loss = criterion(predictions, batch_labels.view(-1, 1))\n",
        "                val_loss += loss.item()\n",
        "            val_loss_avg = val_loss / len(val_dataloader)\n",
        "            scheduler.step(val_loss_avg)  # Adjust learning rate based on validation loss\n",
        "            print(f\"Epoch {epoch + 1}, Validation Loss: {val_loss_avg}\")\n",
        "\n",
        "        # Save losses for later analysis\n",
        "        train_losses_ncf.append(total_loss / len(dataloader))\n",
        "        val_losses_ncf.append(val_loss_avg)\n",
        "\n",
        "        # Evaluate model's performance\n",
        "        recall, ndcg = model_evaluation(model_ncf, val_dict, device, K=10)\n",
        "        recalls_ncf.append(recall)\n",
        "        ndcgs_ncf.append(ndcg)\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss_avg < best_val_loss:\n",
        "            best_val_loss = val_loss_avg\n",
        "            counter = 0\n",
        "            # Save the best model\n",
        "            torch.save(model_ncf.state_dict(), f\"./best_model_ncf_{negative_num}.pth\")\n",
        "        else:\n",
        "            counter += 1\n",
        "            print(f\"Early Stopping Counter: {counter}/{patience}\")\n",
        "            if counter >= patience:\n",
        "                print(\"Early stopping triggered! Stopping training.\")\n",
        "                break\n",
        "\n",
        "    # Save the results for the current negative_num\n",
        "    results[negative_num] = {\n",
        "        \"train_losses\": train_losses_ncf,\n",
        "        \"val_losses\": val_losses_ncf,\n",
        "        \"recalls\": recalls_ncf,\n",
        "        \"ndcgs\": ndcgs_ncf\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pfXy1lPmn6HY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv5I06CXn6YK"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_dict, val_dict, test_dict, movie_num, user_num = load_data_rate('/content/ratings.dat', threshold=3)\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "\n",
        "negative_nums = [20]  # Different values of negative samples to test\n",
        "\n",
        "# Loop through each negative_num\n",
        "for negative_num in negative_nums:\n",
        "    # Update non-interacted movies list for the current negative_num\n",
        "    non_interacted_movies = get_non_interacted_movies(train_dict, val_dict, test_dict, movie_num)\n",
        "\n",
        "    # Get input data for training with the current negative_num\n",
        "    user_input, movie_input, labels = get_input_data(train_dict, non_interacted_movies, negative_num)\n",
        "\n",
        "    # Reinitialize the model, optimizer, and other components\n",
        "    model_ncf = NeuMF(user_num + 1, movie_num + 1, 10, [10, 16]).to(device)\n",
        "    model_ncf = torch.compile(model_ncf)\n",
        "    optimizer = optim.Adam(model_ncf.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=3, verbose=True)\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()  # Loss function\n",
        "    scaler = torch.amp.GradScaler('cuda')  # Automatic mixed precision scaler\n",
        "\n",
        "    # Convert input data to tensors and load them to device\n",
        "    user_input = torch.tensor(user_input, dtype=torch.long).to(device)\n",
        "    movie_input = torch.tensor(movie_input, dtype=torch.long).to(device)\n",
        "    labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(user_input, movie_input, labels)\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "    # Prepare validation data\n",
        "    val_user_input, val_movie_input, val_labels = get_input_data_nointeract(val_dict, non_interacted_movies)\n",
        "    val_dict = defaultdict(list)\n",
        "\n",
        "    # Organize validation data\n",
        "    for user_id, movie_id, label in zip(val_user_input, val_movie_input, val_labels):\n",
        "        val_dict[user_id].append((movie_id, label))\n",
        "\n",
        "    val_dict = dict(val_dict)\n",
        "\n",
        "    # Convert validation data to tensors\n",
        "    val_user_input = torch.tensor(val_user_input, dtype=torch.long).to(device)\n",
        "    val_movie_input = torch.tensor(val_movie_input, dtype=torch.long).to(device)\n",
        "    val_labels = torch.tensor(val_labels, dtype=torch.float32).to(device)\n",
        "\n",
        "    val_dataset = torch.utils.data.TensorDataset(val_user_input, val_movie_input, val_labels)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "\n",
        "    # Start training for the current negative_num\n",
        "    train_losses_ncf = []  # List to store training losses\n",
        "    val_losses_ncf = []  # List to store validation losses\n",
        "    recalls_ncf = []  # List to store recall scores\n",
        "    ndcgs_ncf = []  # List to store NDCG scores\n",
        "    patience = 10  # Patience for early stopping\n",
        "    counter = 0  # Counter for early stopping\n",
        "    best_val_loss = float('inf')  # Best validation loss\n",
        "\n",
        "    # Loop through epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        model_ncf.train()  # Set model to training mode\n",
        "        total_loss = 0\n",
        "\n",
        "        # Train the model with the training data\n",
        "        for batch_users, batch_items, batch_labels in dataloader:\n",
        "            batch_users = batch_users.to(device)\n",
        "            batch_items = batch_items.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            with torch.amp.autocast('cuda'):\n",
        "                predictions = model_ncf(batch_users, batch_items)\n",
        "                loss = criterion(predictions, batch_labels.view(-1, 1))\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Training Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "        # Validate the model\n",
        "        model_ncf.eval()  # Set model to evaluation mode\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_users, batch_items, batch_labels in val_dataloader:\n",
        "                batch_users = batch_users.to(device)\n",
        "                batch_items = batch_items.to(device)\n",
        "                batch_labels = batch_labels.to(device)\n",
        "                predictions = model_ncf(batch_users, batch_items)\n",
        "                loss = criterion(predictions, batch_labels.view(-1, 1))\n",
        "                val_loss += loss.item()\n",
        "            val_loss_avg = val_loss / len(val_dataloader)\n",
        "            scheduler.step(val_loss_avg)  # Adjust learning rate based on validation loss\n",
        "            print(f\"Epoch {epoch + 1}, Validation Loss: {val_loss_avg}\")\n",
        "\n",
        "        # Save losses for later analysis\n",
        "        train_losses_ncf.append(total_loss / len(dataloader))\n",
        "        val_losses_ncf.append(val_loss_avg)\n",
        "\n",
        "        # Evaluate model's performance\n",
        "        recall, ndcg = model_evaluation(model_ncf, val_dict, device, K=10)\n",
        "        recalls_ncf.append(recall)\n",
        "        ndcgs_ncf.append(ndcg)\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss_avg < best_val_loss:\n",
        "            best_val_loss = val_loss_avg\n",
        "            counter = 0\n",
        "            # Save the best model\n",
        "            torch.save(model_ncf.state_dict(), f\"./best_model_ncf_{negative_num}.pth\")\n",
        "        else:\n",
        "            counter += 1\n",
        "            print(f\"Early Stopping Counter: {counter}/{patience}\")\n",
        "            if counter >= patience:\n",
        "                print(\"Early stopping triggered! Stopping training.\")\n",
        "                break\n",
        "\n",
        "    # Save the results for the current negative_num\n",
        "    results[negative_num] = {\n",
        "        \"train_losses\": train_losses_ncf,\n",
        "        \"val_losses\": val_losses_ncf,\n",
        "        \"recalls\": recalls_ncf,\n",
        "        \"ndcgs\": ndcgs_ncf\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.keys())"
      ],
      "metadata": {
        "id": "hXGlKb9EMYeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFwSdg1g9nmB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = np.arange(1, num_epochs + 1)\n",
        "negative_nums = [5, 10, 15, 20]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for negative_num in negative_nums:\n",
        "    train_losses = results[negative_num][\"train_losses\"]\n",
        "    train_losses = np.concatenate([train_losses, [np.nan] * (len(epochs) - len(train_losses))]) if len(train_losses) < len(epochs) else train_losses\n",
        "    plt.plot(epochs, train_losses, linestyle='--', linewidth=2, label=f'Train Loss (Negative Num: {negative_num})')\n",
        "\n",
        "plt.title('Training Loss Across Different Negative Samples')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for negative_num in negative_nums:\n",
        "    val_losses = results[negative_num][\"val_losses\"]\n",
        "    val_losses = np.concatenate([val_losses, [np.nan] * (len(epochs) - len(val_losses))]) if len(val_losses) < len(epochs) else val_losses\n",
        "    plt.plot(epochs, val_losses, linestyle='-.', linewidth=2, label=f'Validation Loss (Negative Num: {negative_num})')\n",
        "\n",
        "plt.title('Validation Loss Across Different Negative Samples')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdy6z3ScoUsX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = np.arange(1, num_epochs + 1)\n",
        "negative_nums = [5, 10, 15, 20]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for negative_num in negative_nums:\n",
        "    recalls = results[negative_num][\"recalls\"]\n",
        "    recalls = np.concatenate([recalls, [np.nan] * (len(epochs) - len(recalls))]) if len(recalls) < len(epochs) else recalls\n",
        "    plt.plot(epochs, recalls, linestyle='-', linewidth=2, label=f'Recall (Negative Num: {negative_num})')\n",
        "\n",
        "plt.title('Recall Across Different Negative Samples')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "for negative_num in negative_nums:\n",
        "    ndcgs = results[negative_num][\"ndcgs\"]\n",
        "    ndcgs = np.concatenate([ndcgs, [np.nan] * (len(epochs) - len(ndcgs))]) if len(ndcgs) < len(epochs) else ndcgs\n",
        "    plt.plot(epochs, ndcgs, linestyle='-', linewidth=2, label=f'NDCG (Negative Num: {negative_num})')\n",
        "\n",
        "plt.title('NDCG Across Different Negative Samples')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('NDCG')\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3VAbIDaoUsX",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# Assuming the device is set and model architectures are defined\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the NeuMF model (replace with your actual architecture)\n",
        "model_ncf = NeuMF(user_num+1, movie_num+1, 10, [10, 16]).to(device)\n",
        "\n",
        "# Define the negative_num or other variable to load the model\n",
        "negative_nums = [20]  # Example negative numbers, change as needed\n",
        "\n",
        "# Prepare storage for recall and NDCG scores\n",
        "recall_scores = []\n",
        "ndcg_scores = []\n",
        "\n",
        "# Loop through each negative_num to load and evaluate models\n",
        "for negative_num in negative_nums:\n",
        "    # Load the model using the respective file name\n",
        "    model_ncf.load_state_dict(torch.load(f\"./best_model_ncf_{negative_num}.pth\"))\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model_ncf.eval()\n",
        "\n",
        "    # Prepare the test data\n",
        "    test_user_input, test_movie_input, test_labels = get_input_data_nointeract(test_dict, non_interacted_movies)\n",
        "    test_dict = defaultdict(list)\n",
        "\n",
        "    for user_id, movie_id, label in zip(test_user_input, test_movie_input, test_labels):\n",
        "        test_dict[user_id].append((movie_id, label))\n",
        "\n",
        "    test_dict = dict(test_dict)\n",
        "\n",
        "    # Evaluate the model on the test data\n",
        "    recall_ncf_test, ndcg_ncf_test = model_evaluation(model_ncf, test_dict, device, K=10)\n",
        "\n",
        "    # Append the results to the lists\n",
        "    recall_scores.append(recall_ncf_test)\n",
        "    ndcg_scores.append(ndcg_ncf_test)\n",
        "\n",
        "# Models (Negative numbers corresponding to the models)\n",
        "models = [f'NCF (neg_num={num})' for num in negative_nums]\n",
        "\n",
        "# Create a bar plot comparing Recall and NDCG scores\n",
        "x = np.arange(len(models))\n",
        "width = 0.4\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 5))\n",
        "\n",
        "# Plot Recall\n",
        "ax1.bar(x - width/2, recall_scores, width, color='blue')\n",
        "ax1.set_ylabel('Recall@10', fontsize=12, fontweight='bold')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(models, fontsize=11)\n",
        "ax1.set_title('Recall Comparison', fontsize=14, fontweight='bold')\n",
        "ax1.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "# Plot NDCG\n",
        "ax2.bar(x + width/2, ndcg_scores, width, color='green')\n",
        "ax2.set_ylabel('NDCG@10', fontsize=12, fontweight='bold')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(models, fontsize=11)\n",
        "ax2.set_title('NDCG Comparison', fontsize=14, fontweight='bold')\n",
        "ax2.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aqM6XuyQoUsX"
      },
      "outputs": [],
      "source": [
        "# print(recall_scores)\n",
        "# print(ndcg_scores)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 6891228,
          "isSourceIdPinned": false,
          "sourceId": 11060274,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6924481,
          "sourceId": 11107019,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}